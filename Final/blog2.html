<!DOCTYPE html>
<head>
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/stylesheet.css" rel="stylesheet">
  <title>Build you own web crawler &#124; Archer's Blog</title>
  <style>
  </style>
</head>
<body>
  <header>
    <h1>Archer's Blog</h1>
    <div class="search hidden-xs hidden-sm">
      <input type="search" placeholder="Search For You Interested" class="form-control" style="width:300px">
      <div style="width:77px;margin-left: 300px;margin-top: -34px">
        <input class="btn btn-default" type="submit" value="Search!" >
      </div>
    </div>
    <br>
    <ul>
      You are the 12345 vistors of this site
    </ul>
  </header>
  <div class="bg_main">
    <div class="bg_nav"><div class="bg_nav_right"><a href="index.html">&#60;&#60;return to index</a></div></div>
    <h2>Build you own web crawler</h2>
    <p>&nbsp;&nbsp;&nbsp;&nbsp;Web crawler to spider called network, network robots, which is a program that will automatically crawls the web on the Internet through the network, this technique may generally be used to check all the links on your site is whether it is valid. Of course, more advanced technology is to save the relevant data down the page, you can become a search engine.

    <br><br>&nbsp;&nbsp;&nbsp;&nbsp;From the technical phase, achieving crawl the web may not be a very difficult thing, difficult thing is to analyze and organize Web pages, it is a need for intelligent lightweight, the program requires a lot of mathematical calculations to do thing. Here's a simple process:
    <img src="img/blog2.png">
    <br><br>&nbsp;&nbsp;&nbsp;&nbsp;Here, we just talk about how to write a web crawler.

          First we look at how to use the command line to come to open the page.

    <br><br>&nbsp;&nbsp;&nbsp;&nbsp;
            telnet somesite.com 80
            <br>&nbsp;&nbsp;&nbsp;&nbsp;GET /index.html HTTP / 1.0
            <br>&nbsp;&nbsp;&nbsp;&nbsp;Press Enter twice

    <br><br>&nbsp;&nbsp;&nbsp;&nbsp;Use telnet is to tell you this is actually a socket technology, and uses HTTP protocol, such as GET method to get web pages, of course, the next thing you'll need to parse the HTML grammar, and even need to parse the Javascript, because the current page using Ajax's more and more, and a lot of web content is loaded through Ajax technology, because, simply parse the HTML file in the future will be far from enough. Of course, here, just to show a very simple crawl, so simple that only as an example, following the example of the pseudo-code:</p>
    <pre>
for each link in (all the links of the current page)
{
        if(the link is the one I want || the link is never accessed)
        {
                process the link
                set the link as accessed
        }
}
</pre>
    <a href="http://coolshell.cn/">Source</a><br>
    <a href="#">Return to top</a>
    <div></div>
    <div class="bg_nav">
      <div class="bg_nav_left"><a href="blog.html">&#60;&#60;Last blog</a></div>
      <div class="bg_nav_right">No more blogs&#62;&#62;</div>
    </div>
  </div>

</body>
